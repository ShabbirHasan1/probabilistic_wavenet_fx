{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 4th root for embedding: https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the combined forex and economic news data\n",
    "df = pd.read_csv('fx_with_news.csv', header=[0,1], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">c_eur</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">after_counter_ohe</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>bar_len</th>\n",
       "      <th>bar_spearman</th>\n",
       "      <th>bar_log_r</th>\n",
       "      <th>first_r</th>\n",
       "      <th>max_r</th>\n",
       "      <th>min_r</th>\n",
       "      <th>last_r</th>\n",
       "      <th>bar_quantile_25_r</th>\n",
       "      <th>...</th>\n",
       "      <th>_united states nondefense capital goods orders ex aircraft</th>\n",
       "      <th>_united states nonfarm payrolls</th>\n",
       "      <th>_united states retail sales control group</th>\n",
       "      <th>_united states retail sales ex autos mom</th>\n",
       "      <th>_united states retail sales mom</th>\n",
       "      <th>_united states reutersmichigan consumer sentiment index</th>\n",
       "      <th>_united states trade balance</th>\n",
       "      <th>_united states unemployment rate</th>\n",
       "      <th>_usd event</th>\n",
       "      <th>_usd speech</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:00:00</th>\n",
       "      <td>1.087467</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.737459</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:05:00</th>\n",
       "      <td>1.087362</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>155.0</td>\n",
       "      <td>-0.181051</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:10:00</th>\n",
       "      <td>1.086980</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>138.0</td>\n",
       "      <td>-0.460522</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:15:00</th>\n",
       "      <td>1.086938</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.282268</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:20:00</th>\n",
       "      <td>1.087057</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>115.0</td>\n",
       "      <td>-0.043127</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:35:00</th>\n",
       "      <td>1.121460</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.800736</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972569</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:40:00</th>\n",
       "      <td>1.121460</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.141550</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:45:00</th>\n",
       "      <td>1.121388</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.612689</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:50:00</th>\n",
       "      <td>1.121327</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-0.912678</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.971528</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:55:00</th>\n",
       "      <td>1.121247</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.075062</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.971181</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298829 rows × 459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        c_eur                                           \\\n",
       "                         mean       std bar_len bar_spearman bar_log_r   \n",
       "ctime                                                                    \n",
       "2016-01-03 22:00:00  1.087467  0.000015    28.0    -0.737459 -0.000037   \n",
       "2016-01-03 22:05:00  1.087362  0.000049   155.0    -0.181051 -0.000230   \n",
       "2016-01-03 22:10:00  1.086980  0.000105   138.0    -0.460522 -0.000248   \n",
       "2016-01-03 22:15:00  1.086938  0.000043   125.0     0.282268 -0.000028   \n",
       "2016-01-03 22:20:00  1.087057  0.000148   115.0    -0.043127 -0.000120   \n",
       "...                       ...       ...     ...          ...       ...   \n",
       "2019-12-31 21:35:00  1.121460  0.000037    62.0    -0.800736 -0.000080   \n",
       "2019-12-31 21:40:00  1.121460  0.000028    41.0    -0.141550 -0.000009   \n",
       "2019-12-31 21:45:00  1.121388  0.000046   108.0     0.612689 -0.000018   \n",
       "2019-12-31 21:50:00  1.121327  0.000046    97.0    -0.912678 -0.000143   \n",
       "2019-12-31 21:55:00  1.121247  0.000131    91.0     0.075062  0.000294   \n",
       "\n",
       "                                                                               \\\n",
       "                      first_r     max_r     min_r    last_r bar_quantile_25_r   \n",
       "ctime                                                                           \n",
       "2016-01-03 22:00:00  0.000030  0.000030 -0.000016 -0.000007         -0.000009   \n",
       "2016-01-03 22:05:00  0.000081  0.000081 -0.000149 -0.000149         -0.000029   \n",
       "2016-01-03 22:10:00  0.000203  0.000203 -0.000147 -0.000046         -0.000064   \n",
       "2016-01-03 22:15:00 -0.000007  0.000057 -0.000108 -0.000035         -0.000007   \n",
       "2016-01-03 22:20:00 -0.000144  0.000224 -0.000264 -0.000264         -0.000126   \n",
       "...                       ...       ...       ...       ...               ...   \n",
       "2019-12-31 21:35:00  0.000062  0.000062 -0.000036 -0.000018         -0.000027   \n",
       "2019-12-31 21:40:00 -0.000027  0.000035 -0.000054 -0.000036         -0.000018   \n",
       "2019-12-31 21:45:00  0.000029  0.000047 -0.000096  0.000011         -0.000025   \n",
       "2019-12-31 21:50:00  0.000056  0.000092 -0.000122 -0.000086         -0.000015   \n",
       "2019-12-31 21:55:00 -0.000033  0.000261 -0.000123  0.000261         -0.000078   \n",
       "\n",
       "                     ...  \\\n",
       "                     ...   \n",
       "ctime                ...   \n",
       "2016-01-03 22:00:00  ...   \n",
       "2016-01-03 22:05:00  ...   \n",
       "2016-01-03 22:10:00  ...   \n",
       "2016-01-03 22:15:00  ...   \n",
       "2016-01-03 22:20:00  ...   \n",
       "...                  ...   \n",
       "2019-12-31 21:35:00  ...   \n",
       "2019-12-31 21:40:00  ...   \n",
       "2019-12-31 21:45:00  ...   \n",
       "2019-12-31 21:50:00  ...   \n",
       "2019-12-31 21:55:00  ...   \n",
       "\n",
       "                                                             after_counter_ohe  \\\n",
       "                    _united states nondefense capital goods orders ex aircraft   \n",
       "ctime                                                                            \n",
       "2016-01-03 22:00:00                                           0.000000           \n",
       "2016-01-03 22:05:00                                           0.000000           \n",
       "2016-01-03 22:10:00                                           0.000000           \n",
       "2016-01-03 22:15:00                                           0.000000           \n",
       "2016-01-03 22:20:00                                           0.000000           \n",
       "...                                                                ...           \n",
       "2019-12-31 21:35:00                                           0.429514           \n",
       "2019-12-31 21:40:00                                           0.429167           \n",
       "2019-12-31 21:45:00                                           0.428819           \n",
       "2019-12-31 21:50:00                                           0.428472           \n",
       "2019-12-31 21:55:00                                           0.428125           \n",
       "\n",
       "                                                     \\\n",
       "                    _united states nonfarm payrolls   \n",
       "ctime                                                 \n",
       "2016-01-03 22:00:00                             0.0   \n",
       "2016-01-03 22:05:00                             0.0   \n",
       "2016-01-03 22:10:00                             0.0   \n",
       "2016-01-03 22:15:00                             0.0   \n",
       "2016-01-03 22:20:00                             0.0   \n",
       "...                                             ...   \n",
       "2019-12-31 21:35:00                             0.0   \n",
       "2019-12-31 21:40:00                             0.0   \n",
       "2019-12-31 21:45:00                             0.0   \n",
       "2019-12-31 21:50:00                             0.0   \n",
       "2019-12-31 21:55:00                             0.0   \n",
       "\n",
       "                                                               \\\n",
       "                    _united states retail sales control group   \n",
       "ctime                                                           \n",
       "2016-01-03 22:00:00                                       0.0   \n",
       "2016-01-03 22:05:00                                       0.0   \n",
       "2016-01-03 22:10:00                                       0.0   \n",
       "2016-01-03 22:15:00                                       0.0   \n",
       "2016-01-03 22:20:00                                       0.0   \n",
       "...                                                       ...   \n",
       "2019-12-31 21:35:00                                       0.0   \n",
       "2019-12-31 21:40:00                                       0.0   \n",
       "2019-12-31 21:45:00                                       0.0   \n",
       "2019-12-31 21:50:00                                       0.0   \n",
       "2019-12-31 21:55:00                                       0.0   \n",
       "\n",
       "                                                              \\\n",
       "                    _united states retail sales ex autos mom   \n",
       "ctime                                                          \n",
       "2016-01-03 22:00:00                                      0.0   \n",
       "2016-01-03 22:05:00                                      0.0   \n",
       "2016-01-03 22:10:00                                      0.0   \n",
       "2016-01-03 22:15:00                                      0.0   \n",
       "2016-01-03 22:20:00                                      0.0   \n",
       "...                                                      ...   \n",
       "2019-12-31 21:35:00                                      0.0   \n",
       "2019-12-31 21:40:00                                      0.0   \n",
       "2019-12-31 21:45:00                                      0.0   \n",
       "2019-12-31 21:50:00                                      0.0   \n",
       "2019-12-31 21:55:00                                      0.0   \n",
       "\n",
       "                                                     \\\n",
       "                    _united states retail sales mom   \n",
       "ctime                                                 \n",
       "2016-01-03 22:00:00                             0.0   \n",
       "2016-01-03 22:05:00                             0.0   \n",
       "2016-01-03 22:10:00                             0.0   \n",
       "2016-01-03 22:15:00                             0.0   \n",
       "2016-01-03 22:20:00                             0.0   \n",
       "...                                             ...   \n",
       "2019-12-31 21:35:00                             0.0   \n",
       "2019-12-31 21:40:00                             0.0   \n",
       "2019-12-31 21:45:00                             0.0   \n",
       "2019-12-31 21:50:00                             0.0   \n",
       "2019-12-31 21:55:00                             0.0   \n",
       "\n",
       "                                                                             \\\n",
       "                    _united states reutersmichigan consumer sentiment index   \n",
       "ctime                                                                         \n",
       "2016-01-03 22:00:00                                                0.0        \n",
       "2016-01-03 22:05:00                                                0.0        \n",
       "2016-01-03 22:10:00                                                0.0        \n",
       "2016-01-03 22:15:00                                                0.0        \n",
       "2016-01-03 22:20:00                                                0.0        \n",
       "...                                                                ...        \n",
       "2019-12-31 21:35:00                                                0.0        \n",
       "2019-12-31 21:40:00                                                0.0        \n",
       "2019-12-31 21:45:00                                                0.0        \n",
       "2019-12-31 21:50:00                                                0.0        \n",
       "2019-12-31 21:55:00                                                0.0        \n",
       "\n",
       "                                                  \\\n",
       "                    _united states trade balance   \n",
       "ctime                                              \n",
       "2016-01-03 22:00:00                          0.0   \n",
       "2016-01-03 22:05:00                          0.0   \n",
       "2016-01-03 22:10:00                          0.0   \n",
       "2016-01-03 22:15:00                          0.0   \n",
       "2016-01-03 22:20:00                          0.0   \n",
       "...                                          ...   \n",
       "2019-12-31 21:35:00                          0.0   \n",
       "2019-12-31 21:40:00                          0.0   \n",
       "2019-12-31 21:45:00                          0.0   \n",
       "2019-12-31 21:50:00                          0.0   \n",
       "2019-12-31 21:55:00                          0.0   \n",
       "\n",
       "                                                                             \n",
       "                    _united states unemployment rate _usd event _usd speech  \n",
       "ctime                                                                        \n",
       "2016-01-03 22:00:00                              0.0   0.000000         0.0  \n",
       "2016-01-03 22:05:00                              0.0   0.000000         0.0  \n",
       "2016-01-03 22:10:00                              0.0   0.000000         0.0  \n",
       "2016-01-03 22:15:00                              0.0   0.000000         0.0  \n",
       "2016-01-03 22:20:00                              0.0   0.000000         0.0  \n",
       "...                                              ...        ...         ...  \n",
       "2019-12-31 21:35:00                              0.0   0.972569         0.0  \n",
       "2019-12-31 21:40:00                              0.0   0.972222         0.0  \n",
       "2019-12-31 21:45:00                              0.0   0.971875         0.0  \n",
       "2019-12-31 21:50:00                              0.0   0.971528         0.0  \n",
       "2019-12-31 21:55:00                              0.0   0.971181         0.0  \n",
       "\n",
       "[298829 rows x 459 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of currencies\n",
    "c_list = ['c_eur', 'c_gbp', 'c_jpy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new feature for more stable forecast\n",
    "for cur in c_list:    \n",
    "    # logreturn of the means\n",
    "    df[(cur,'mean_log_r')] = np.log(1.0 + df[(cur,'mean')].pct_change())\n",
    "    df[(cur,'mean_log_r')] = df[(cur,'mean_log_r')].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "TOP COLUMN:  c_eur\n",
      "SUBCOLUMNS:  12\n",
      "['mean', 'std', 'bar_len', 'bar_spearman', 'bar_log_r', 'first_r', 'max_r', 'min_r', 'last_r', 'bar_quantile_25_r', 'bar_quantile_75_r', 'mean_log_r']\n",
      "----------------------------------\n",
      "TOP COLUMN:  c_gbp\n",
      "SUBCOLUMNS:  12\n",
      "['mean', 'std', 'bar_len', 'bar_spearman', 'bar_log_r', 'first_r', 'max_r', 'min_r', 'last_r', 'bar_quantile_25_r', 'bar_quantile_75_r', 'mean_log_r']\n",
      "----------------------------------\n",
      "TOP COLUMN:  c_jpy\n",
      "SUBCOLUMNS:  12\n",
      "['mean', 'std', 'bar_len', 'bar_spearman', 'bar_log_r', 'first_r', 'max_r', 'min_r', 'last_r', 'bar_quantile_25_r', 'bar_quantile_75_r', 'mean_log_r']\n",
      "----------------------------------\n",
      "TOP COLUMN:  month\n",
      "SUBCOLUMNS:  12\n",
      "['_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10', '_11', '_12']\n",
      "----------------------------------\n",
      "TOP COLUMN:  dow\n",
      "SUBCOLUMNS:  6\n",
      "['_0', '_1', '_2', '_3', '_4', '_6']\n",
      "----------------------------------\n",
      "TOP COLUMN:  hour\n",
      "SUBCOLUMNS:  24\n",
      "['_0', '_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10', '_11', '_12', '_13', '_14', '_15', '_16', '_17', '_18', '_19', '_20', '_21', '_22', '_23']\n",
      "----------------------------------\n",
      "TOP COLUMN:  event_cur\n",
      "SUBCOLUMNS:  4\n",
      "['_EUR', '_GBP', '_JPY', '_USD']\n",
      "----------------------------------\n",
      "TOP COLUMN:  event_exist\n",
      "SUBCOLUMNS:  76\n",
      "['_eur event', '_eur speech', '_european monetary union consumer price index  core yoy', '_european monetary union consumer price index yoy', '_european monetary union ecb deposit rate decision', '_european monetary union ecb interest rate decision', '_european monetary union gross domestic product sa qoq', '_european monetary union gross domestic product sa yoy', '_european monetary union markit pmi composite', '_france markit manufacturing pmi', '_gbp event', '_gbp speech', '_germany gross domestic product qoq', '_germany harmonised index of consumer prices yoy', '_germany harmonized index of consumer prices yoy', '_germany markit manufacturing pmi', '_germany unemployment change', '_germany unemployment rate sa', '_germany zew survey  economic sentiment', '_japan boj interest rate decision', '_japan current account nsa', '_japan gross domestic product qoq', '_japan tokyo cpi ex fresh food yoy', '_jpy event', '_jpy speech', '_united kingdom average earnings excluding bonus 3moyr', '_united kingdom average earnings including bonus 3moyr', '_united kingdom boe asset purchase facility', '_united kingdom boe interest rate decision', '_united kingdom boe mpc vote cut', '_united kingdom boe mpc vote hike', '_united kingdom boe mpc vote unchanged', '_united kingdom consumer inflation expectations', '_united kingdom consumer price index yoy', '_united kingdom core consumer price index yoy', '_united kingdom gross domestic product mom', '_united kingdom gross domestic product qoq', '_united kingdom ilo unemployment rate 3m', '_united kingdom niesr gdp estimate 3m', '_united states adp employment change', '_united states average hourly earnings yoy', '_united states building permits change', '_united states building permits mom', '_united states consumer confidence', '_united states consumer price index ex food  energy mom', '_united states consumer price index ex food  energy yoy', '_united states consumer price index yoy', '_united states core personal consumption expenditure  price index mom', '_united states core personal consumption expenditure  price index yoy', '_united states core personal consumption expenditures qoq', '_united states durable goods orders', '_united states durable goods orders ex defense', '_united states durable goods orders ex transportation', '_united states existing home sales mom', '_united states fed interest rate decision', '_united states goods trade balance', '_united states gross domestic product annualized', '_united states gross domestic product price index', '_united states initial jobless claims', '_united states initial jobless claims 4week average', '_united states ism manufacturing pmi', '_united states ism nonmanufacturing pmi', '_united states ism prices paid', '_united states jolts job openings', '_united states michigan consumer sentiment index', '_united states new home sales mom', '_united states nondefense capital goods orders ex aircraft', '_united states nonfarm payrolls', '_united states retail sales control group', '_united states retail sales ex autos mom', '_united states retail sales mom', '_united states reutersmichigan consumer sentiment index', '_united states trade balance', '_united states unemployment rate', '_usd event', '_usd speech']\n",
      "----------------------------------\n",
      "TOP COLUMN:  actual_ohe\n",
      "SUBCOLUMNS:  76\n",
      "['_eur event', '_eur speech', '_european monetary union consumer price index  core yoy', '_european monetary union consumer price index yoy', '_european monetary union ecb deposit rate decision', '_european monetary union ecb interest rate decision', '_european monetary union gross domestic product sa qoq', '_european monetary union gross domestic product sa yoy', '_european monetary union markit pmi composite', '_france markit manufacturing pmi', '_gbp event', '_gbp speech', '_germany gross domestic product qoq', '_germany harmonised index of consumer prices yoy', '_germany harmonized index of consumer prices yoy', '_germany markit manufacturing pmi', '_germany unemployment change', '_germany unemployment rate sa', '_germany zew survey  economic sentiment', '_japan boj interest rate decision', '_japan current account nsa', '_japan gross domestic product qoq', '_japan tokyo cpi ex fresh food yoy', '_jpy event', '_jpy speech', '_united kingdom average earnings excluding bonus 3moyr', '_united kingdom average earnings including bonus 3moyr', '_united kingdom boe asset purchase facility', '_united kingdom boe interest rate decision', '_united kingdom boe mpc vote cut', '_united kingdom boe mpc vote hike', '_united kingdom boe mpc vote unchanged', '_united kingdom consumer inflation expectations', '_united kingdom consumer price index yoy', '_united kingdom core consumer price index yoy', '_united kingdom gross domestic product mom', '_united kingdom gross domestic product qoq', '_united kingdom ilo unemployment rate 3m', '_united kingdom niesr gdp estimate 3m', '_united states adp employment change', '_united states average hourly earnings yoy', '_united states building permits change', '_united states building permits mom', '_united states consumer confidence', '_united states consumer price index ex food  energy mom', '_united states consumer price index ex food  energy yoy', '_united states consumer price index yoy', '_united states core personal consumption expenditure  price index mom', '_united states core personal consumption expenditure  price index yoy', '_united states core personal consumption expenditures qoq', '_united states durable goods orders', '_united states durable goods orders ex defense', '_united states durable goods orders ex transportation', '_united states existing home sales mom', '_united states fed interest rate decision', '_united states goods trade balance', '_united states gross domestic product annualized', '_united states gross domestic product price index', '_united states initial jobless claims', '_united states initial jobless claims 4week average', '_united states ism manufacturing pmi', '_united states ism nonmanufacturing pmi', '_united states ism prices paid', '_united states jolts job openings', '_united states michigan consumer sentiment index', '_united states new home sales mom', '_united states nondefense capital goods orders ex aircraft', '_united states nonfarm payrolls', '_united states retail sales control group', '_united states retail sales ex autos mom', '_united states retail sales mom', '_united states reutersmichigan consumer sentiment index', '_united states trade balance', '_united states unemployment rate', '_usd event', '_usd speech']\n",
      "----------------------------------\n",
      "TOP COLUMN:  surprise_ohe\n",
      "SUBCOLUMNS:  76\n",
      "['_eur event', '_eur speech', '_european monetary union consumer price index  core yoy', '_european monetary union consumer price index yoy', '_european monetary union ecb deposit rate decision', '_european monetary union ecb interest rate decision', '_european monetary union gross domestic product sa qoq', '_european monetary union gross domestic product sa yoy', '_european monetary union markit pmi composite', '_france markit manufacturing pmi', '_gbp event', '_gbp speech', '_germany gross domestic product qoq', '_germany harmonised index of consumer prices yoy', '_germany harmonized index of consumer prices yoy', '_germany markit manufacturing pmi', '_germany unemployment change', '_germany unemployment rate sa', '_germany zew survey  economic sentiment', '_japan boj interest rate decision', '_japan current account nsa', '_japan gross domestic product qoq', '_japan tokyo cpi ex fresh food yoy', '_jpy event', '_jpy speech', '_united kingdom average earnings excluding bonus 3moyr', '_united kingdom average earnings including bonus 3moyr', '_united kingdom boe asset purchase facility', '_united kingdom boe interest rate decision', '_united kingdom boe mpc vote cut', '_united kingdom boe mpc vote hike', '_united kingdom boe mpc vote unchanged', '_united kingdom consumer inflation expectations', '_united kingdom consumer price index yoy', '_united kingdom core consumer price index yoy', '_united kingdom gross domestic product mom', '_united kingdom gross domestic product qoq', '_united kingdom ilo unemployment rate 3m', '_united kingdom niesr gdp estimate 3m', '_united states adp employment change', '_united states average hourly earnings yoy', '_united states building permits change', '_united states building permits mom', '_united states consumer confidence', '_united states consumer price index ex food  energy mom', '_united states consumer price index ex food  energy yoy', '_united states consumer price index yoy', '_united states core personal consumption expenditure  price index mom', '_united states core personal consumption expenditure  price index yoy', '_united states core personal consumption expenditures qoq', '_united states durable goods orders', '_united states durable goods orders ex defense', '_united states durable goods orders ex transportation', '_united states existing home sales mom', '_united states fed interest rate decision', '_united states goods trade balance', '_united states gross domestic product annualized', '_united states gross domestic product price index', '_united states initial jobless claims', '_united states initial jobless claims 4week average', '_united states ism manufacturing pmi', '_united states ism nonmanufacturing pmi', '_united states ism prices paid', '_united states jolts job openings', '_united states michigan consumer sentiment index', '_united states new home sales mom', '_united states nondefense capital goods orders ex aircraft', '_united states nonfarm payrolls', '_united states retail sales control group', '_united states retail sales ex autos mom', '_united states retail sales mom', '_united states reutersmichigan consumer sentiment index', '_united states trade balance', '_united states unemployment rate', '_usd event', '_usd speech']\n",
      "----------------------------------\n",
      "TOP COLUMN:  change_ohe\n",
      "SUBCOLUMNS:  76\n",
      "['_eur event', '_eur speech', '_european monetary union consumer price index  core yoy', '_european monetary union consumer price index yoy', '_european monetary union ecb deposit rate decision', '_european monetary union ecb interest rate decision', '_european monetary union gross domestic product sa qoq', '_european monetary union gross domestic product sa yoy', '_european monetary union markit pmi composite', '_france markit manufacturing pmi', '_gbp event', '_gbp speech', '_germany gross domestic product qoq', '_germany harmonised index of consumer prices yoy', '_germany harmonized index of consumer prices yoy', '_germany markit manufacturing pmi', '_germany unemployment change', '_germany unemployment rate sa', '_germany zew survey  economic sentiment', '_japan boj interest rate decision', '_japan current account nsa', '_japan gross domestic product qoq', '_japan tokyo cpi ex fresh food yoy', '_jpy event', '_jpy speech', '_united kingdom average earnings excluding bonus 3moyr', '_united kingdom average earnings including bonus 3moyr', '_united kingdom boe asset purchase facility', '_united kingdom boe interest rate decision', '_united kingdom boe mpc vote cut', '_united kingdom boe mpc vote hike', '_united kingdom boe mpc vote unchanged', '_united kingdom consumer inflation expectations', '_united kingdom consumer price index yoy', '_united kingdom core consumer price index yoy', '_united kingdom gross domestic product mom', '_united kingdom gross domestic product qoq', '_united kingdom ilo unemployment rate 3m', '_united kingdom niesr gdp estimate 3m', '_united states adp employment change', '_united states average hourly earnings yoy', '_united states building permits change', '_united states building permits mom', '_united states consumer confidence', '_united states consumer price index ex food  energy mom', '_united states consumer price index ex food  energy yoy', '_united states consumer price index yoy', '_united states core personal consumption expenditure  price index mom', '_united states core personal consumption expenditure  price index yoy', '_united states core personal consumption expenditures qoq', '_united states durable goods orders', '_united states durable goods orders ex defense', '_united states durable goods orders ex transportation', '_united states existing home sales mom', '_united states fed interest rate decision', '_united states goods trade balance', '_united states gross domestic product annualized', '_united states gross domestic product price index', '_united states initial jobless claims', '_united states initial jobless claims 4week average', '_united states ism manufacturing pmi', '_united states ism nonmanufacturing pmi', '_united states ism prices paid', '_united states jolts job openings', '_united states michigan consumer sentiment index', '_united states new home sales mom', '_united states nondefense capital goods orders ex aircraft', '_united states nonfarm payrolls', '_united states retail sales control group', '_united states retail sales ex autos mom', '_united states retail sales mom', '_united states reutersmichigan consumer sentiment index', '_united states trade balance', '_united states unemployment rate', '_usd event', '_usd speech']\n",
      "----------------------------------\n",
      "TOP COLUMN:  after_counter_ohe\n",
      "SUBCOLUMNS:  76\n",
      "['_eur event', '_eur speech', '_european monetary union consumer price index  core yoy', '_european monetary union consumer price index yoy', '_european monetary union ecb deposit rate decision', '_european monetary union ecb interest rate decision', '_european monetary union gross domestic product sa qoq', '_european monetary union gross domestic product sa yoy', '_european monetary union markit pmi composite', '_france markit manufacturing pmi', '_gbp event', '_gbp speech', '_germany gross domestic product qoq', '_germany harmonised index of consumer prices yoy', '_germany harmonized index of consumer prices yoy', '_germany markit manufacturing pmi', '_germany unemployment change', '_germany unemployment rate sa', '_germany zew survey  economic sentiment', '_japan boj interest rate decision', '_japan current account nsa', '_japan gross domestic product qoq', '_japan tokyo cpi ex fresh food yoy', '_jpy event', '_jpy speech', '_united kingdom average earnings excluding bonus 3moyr', '_united kingdom average earnings including bonus 3moyr', '_united kingdom boe asset purchase facility', '_united kingdom boe interest rate decision', '_united kingdom boe mpc vote cut', '_united kingdom boe mpc vote hike', '_united kingdom boe mpc vote unchanged', '_united kingdom consumer inflation expectations', '_united kingdom consumer price index yoy', '_united kingdom core consumer price index yoy', '_united kingdom gross domestic product mom', '_united kingdom gross domestic product qoq', '_united kingdom ilo unemployment rate 3m', '_united kingdom niesr gdp estimate 3m', '_united states adp employment change', '_united states average hourly earnings yoy', '_united states building permits change', '_united states building permits mom', '_united states consumer confidence', '_united states consumer price index ex food  energy mom', '_united states consumer price index ex food  energy yoy', '_united states consumer price index yoy', '_united states core personal consumption expenditure  price index mom', '_united states core personal consumption expenditure  price index yoy', '_united states core personal consumption expenditures qoq', '_united states durable goods orders', '_united states durable goods orders ex defense', '_united states durable goods orders ex transportation', '_united states existing home sales mom', '_united states fed interest rate decision', '_united states goods trade balance', '_united states gross domestic product annualized', '_united states gross domestic product price index', '_united states initial jobless claims', '_united states initial jobless claims 4week average', '_united states ism manufacturing pmi', '_united states ism nonmanufacturing pmi', '_united states ism prices paid', '_united states jolts job openings', '_united states michigan consumer sentiment index', '_united states new home sales mom', '_united states nondefense capital goods orders ex aircraft', '_united states nonfarm payrolls', '_united states retail sales control group', '_united states retail sales ex autos mom', '_united states retail sales mom', '_united states reutersmichigan consumer sentiment index', '_united states trade balance', '_united states unemployment rate', '_usd event', '_usd speech']\n"
     ]
    }
   ],
   "source": [
    "# column hierarchy\n",
    "for top in list(df.columns.get_level_values(0).unique()):\n",
    "    print('----------------------------------')\n",
    "    print('TOP COLUMN: ', top)\n",
    "    print('SUBCOLUMNS: ', len(df[top].columns))\n",
    "    print(list(df[top].columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c_eur',\n",
       " 'c_gbp',\n",
       " 'c_jpy',\n",
       " 'month',\n",
       " 'dow',\n",
       " 'hour',\n",
       " 'event_cur',\n",
       " 'event_exist',\n",
       " 'actual_ohe',\n",
       " 'surprise_ohe',\n",
       " 'change_ohe',\n",
       " 'after_counter_ohe']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_level = list(df.columns.get_level_values(0).unique())\n",
    "top_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use 2016-2018 as training data, and 2019 as validation and test data\n",
    "# problem: we have events that occure only in 2019\n",
    "# we can't normalize/standardize/scale them based on 2016-2018 data\n",
    "# we could move them to the '_(currency) event' columns, \n",
    "# but there is only a few, so here I will delete them\n",
    "\n",
    "# have we all event type in 2016-2018 yers?\n",
    "df['event_exist'].loc['2016':'2019'].any().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_european monetary union markit pmi composite',\n",
       " '_germany zew survey  economic sentiment',\n",
       " '_japan gross domestic product qoq',\n",
       " '_united states consumer price index ex food  energy mom',\n",
       " '_united states initial jobless claims 4week average',\n",
       " '_united states nondefense capital goods orders ex aircraft']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find events occuring only in 2019\n",
    "event_before_19 = df['event_exist'].loc['2016':'2019'].any()\n",
    "event_only_19 = list(event_before_19[event_before_19==False].index)\n",
    "event_only_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop events only occuring in 2019\n",
    "df.drop(event_only_19, axis=1, level=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have we all event type in 2016-2018 yers?\n",
    "df['event_exist'].loc['2016':'2019'].any().all()\n",
    "#ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>c_eur</th>\n",
       "      <th>c_gbp</th>\n",
       "      <th>c_jpy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:00:00</th>\n",
       "      <td>1.087467</td>\n",
       "      <td>1.473700</td>\n",
       "      <td>0.831731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:05:00</th>\n",
       "      <td>1.087362</td>\n",
       "      <td>1.474204</td>\n",
       "      <td>0.831720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:10:00</th>\n",
       "      <td>1.086980</td>\n",
       "      <td>1.474567</td>\n",
       "      <td>0.831713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:15:00</th>\n",
       "      <td>1.086938</td>\n",
       "      <td>1.474208</td>\n",
       "      <td>0.831647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:20:00</th>\n",
       "      <td>1.087057</td>\n",
       "      <td>1.474157</td>\n",
       "      <td>0.831599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:35:00</th>\n",
       "      <td>1.121460</td>\n",
       "      <td>1.324929</td>\n",
       "      <td>0.920382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:40:00</th>\n",
       "      <td>1.121460</td>\n",
       "      <td>1.325454</td>\n",
       "      <td>0.920438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:45:00</th>\n",
       "      <td>1.121388</td>\n",
       "      <td>1.326264</td>\n",
       "      <td>0.920370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:50:00</th>\n",
       "      <td>1.121327</td>\n",
       "      <td>1.326356</td>\n",
       "      <td>0.920378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:55:00</th>\n",
       "      <td>1.121247</td>\n",
       "      <td>1.326194</td>\n",
       "      <td>0.920179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298829 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        c_eur     c_gbp     c_jpy\n",
       "                         mean      mean      mean\n",
       "ctime                                            \n",
       "2016-01-03 22:00:00  1.087467  1.473700  0.831731\n",
       "2016-01-03 22:05:00  1.087362  1.474204  0.831720\n",
       "2016-01-03 22:10:00  1.086980  1.474567  0.831713\n",
       "2016-01-03 22:15:00  1.086938  1.474208  0.831647\n",
       "2016-01-03 22:20:00  1.087057  1.474157  0.831599\n",
       "...                       ...       ...       ...\n",
       "2019-12-31 21:35:00  1.121460  1.324929  0.920382\n",
       "2019-12-31 21:40:00  1.121460  1.325454  0.920438\n",
       "2019-12-31 21:45:00  1.121388  1.326264  0.920370\n",
       "2019-12-31 21:50:00  1.121327  1.326356  0.920378\n",
       "2019-12-31 21:55:00  1.121247  1.326194  0.920179\n",
       "\n",
       "[298829 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rescaling output isn't always important, \n",
    "# but if the model has different output features it helps to weight their effect on the loss function\n",
    "# or we can use multiple loss functions and manually set the weight\n",
    "df_label = df.xs('mean', axis=1, level=1, drop_level=False)\n",
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use only train and validation\n",
    "# Standardizing and normalizing shifts the dataset, where +- sign can be important. Later the model will adjust this by the biases,\n",
    "# but the information is lost. It could improve the performance if we make a new feature for + and - numbers, but here I don't want to make more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spearman doesn't need standardization\n",
    "c_single_features = ['mean', 'std', 'bar_len']\n",
    "feature_scalers = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize features based on data up to 2019-01-01 00:00\n",
    "for cur in c_list:\n",
    "    for feature in c_single_features:\n",
    "        column_mean = df[cur, feature].loc['2016':'2019'].mean()\n",
    "        column_std = df[cur, feature].loc['2016':'2019'].std()\n",
    "        feature_scalers.update({(cur, feature) : [column_mean, column_std]})\n",
    "        df.loc[:,(cur, feature)] = (df[cur, feature] - column_mean)/column_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('c_eur', 'bar_len'): [412.9294628759021, 381.7685744752972],\n",
       " ('c_eur', 'mean'): [1.139248301570798, 0.0499815489615703],\n",
       " ('c_eur', 'std'): [0.00010543836669119138, 9.077071115943524e-05],\n",
       " ('c_gbp', 'bar_len'): [387.5186401063365, 303.3104183503982],\n",
       " ('c_gbp', 'mean'): [1.326336873217585, 0.06648514955410888],\n",
       " ('c_gbp', 'std'): [0.000146642848991849, 0.00015890007722327068],\n",
       " ('c_jpy', 'bar_len'): [354.61210380295637, 299.81779856625644],\n",
       " ('c_jpy', 'mean'): [0.9066954709408044, 0.03345136241766339],\n",
       " ('c_jpy', 'std'): [9.800579913539747e-05, 9.589094629696171e-05]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!! later we can use them to rescale if needed\n",
    "feature_scalers # column_mean, column_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_log_r           1435.641469\n",
      "bar_log_r            1756.307094\n",
      "first_r              1079.158746\n",
      "max_r                1680.077984\n",
      "min_r                1686.788765\n",
      "last_r                960.660734\n",
      "bar_quantile_25_r     647.482120\n",
      "bar_quantile_75_r     647.172621\n",
      "dtype: float64\n",
      "mean_log_r              1.023680\n",
      "bar_log_r               8.380970\n",
      "first_r                -0.840415\n",
      "max_r                1680.077984\n",
      "min_r               -1686.788765\n",
      "last_r                  7.540555\n",
      "bar_quantile_25_r    -647.041955\n",
      "bar_quantile_75_r     646.659518\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "c_related_features = ['mean_log_r','bar_log_r','first_r','max_r','min_r','last_r','bar_quantile_25_r','bar_quantile_75_r']\n",
    "# these are related feature, we can think of them as one feature with subfeatures\n",
    "# they are on similar scale \n",
    "# and the pairs on the other side of the mean (like max_r and min_r) are almost symmetric\n",
    "# to preserve the signs we don't subtract the mean, only divide by the std\n",
    "# this way we preserve their relative size\n",
    "# *10000000 is only for easier comparison\n",
    "print(df['c_eur'][c_related_features].abs().mean()*10000000)\n",
    "print(df['c_eur'][c_related_features].mean()*10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_std = df[cur, 'bar_log_r'].loc['2016':'2019'].std()\n",
    "\n",
    "for cur in c_list:\n",
    "    for feature in c_related_features:\n",
    "        df.loc[:,(cur, feature)] = (df[cur, feature])/related_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003561751218763309"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!! needed later to rescale outputs of log_r features\n",
    "related_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_log_r           0.403072\n",
      "bar_log_r            0.493102\n",
      "first_r              0.302985\n",
      "max_r                0.471700\n",
      "min_r                0.473584\n",
      "last_r               0.269716\n",
      "bar_quantile_25_r    0.181788\n",
      "bar_quantile_75_r    0.181701\n",
      "dtype: float64\n",
      "mean_log_r           0.000287\n",
      "bar_log_r            0.002353\n",
      "first_r             -0.000236\n",
      "max_r                0.471700\n",
      "min_r               -0.473584\n",
      "last_r               0.002117\n",
      "bar_quantile_25_r   -0.181664\n",
      "bar_quantile_75_r    0.181557\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ok, these features remained on similar scale, but the other features as well\n",
    "print(df['c_eur'][c_related_features].abs().mean())\n",
    "print(df['c_eur'][c_related_features].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_EUR    2.000000\n",
       "_GBP    2.828427\n",
       "_JPY    1.732051\n",
       "_USD    3.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'event_cur' is on similar scale, we don't touch (normalization would be better than standardization)\n",
    "df['event_cur'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should consider every event type alone, but there are 76 event types, so just normalize\n",
    "df['actual_ohe'] = (df['actual_ohe'] - df['actual_ohe'].loc['2016':'2019'].min())/(df['actual_ohe'].loc['2016':'2019'].max() - df['actual_ohe'].loc['2016':'2019'].min())\n",
    "# where max-min == 0 we get NaN, fill with 0\n",
    "df['actual_ohe'] = df['actual_ohe'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing with the max of absolute values keeps the signs\n",
    "df['surprise_ohe'] = df['surprise_ohe']/df['surprise_ohe'].loc['2016':'2019'].abs().max()\n",
    "df['surprise_ohe'] = df['surprise_ohe'].fillna(0.0)\n",
    "\n",
    "df['change_ohe'] = df['change_ohe']/df['change_ohe'].loc['2016':'2019'].abs().max()\n",
    "df['change_ohe'] = df['change_ohe'].fillna(0.0)\n",
    "\n",
    "df['after_counter_ohe'] = df['after_counter_ohe']/df['after_counter_ohe'].loc['2016':'2019'].abs().max()\n",
    "df['after_counter_ohe'] = df['after_counter_ohe'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_label = df_label.astype('float16')\n",
    "# df = df.astype('float16')\n",
    "df_valid = df.loc['2019':]\n",
    "df = df.loc[:'2019']\n",
    "df_label_valid = df_label.loc['2019':]\n",
    "df_label = df_label.loc[:'2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1175694609368847\n",
      "0.23388258061315387\n"
     ]
    }
   ],
   "source": [
    "print(df.std().max())\n",
    "print(df.std().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=144\n",
    "shift=1\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label data begins from the end of the first train data\n",
    "\n",
    "def ds_input(df, concat_tops, feature_to_last):\n",
    "    nx = df.to_numpy()\n",
    "    if (isinstance(df.columns, pd.core.index.MultiIndex) and concat_tops==False):\n",
    "        top_level_nb = len(df.columns.get_level_values(0).unique())\n",
    "        sub_level_nb = len(df.columns.get_level_values(1).unique())\n",
    "        nx = nx.reshape(-1,top_level_nb,sub_level_nb)\n",
    "        if feature_to_last == True:\n",
    "            nx = np.moveaxis(nx, [0,1,2], [0,2,1])\n",
    "        nx = np.squeeze(nx)\n",
    "    return nx\n",
    "\n",
    "def make_ds(batch_size, time_steps, shift, skip_steps, df, concat_tops=False, feature_to_last=True):\n",
    "    nx = ds_input(df, concat_tops, feature_to_last)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(nx[skip_steps:])\n",
    "    ds = ds.window(time_steps, shift=shift, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window: window.batch(time_steps))\n",
    "    return ds.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset_args = {'batch_size':batch_size,\n",
    "                      'time_steps':train_size,\n",
    "                      'shift':shift, \n",
    "                      'skip_steps':1}\n",
    "\n",
    "ds_label = make_ds(**label_dataset_args, df=df_label)\n",
    "ds_label_valid = make_ds(**label_dataset_args, df=df_label_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset_args = {'batch_size':batch_size,\n",
    "                      'time_steps':train_size,\n",
    "                      'shift':shift, \n",
    "                      'skip_steps':0}\n",
    "\n",
    "ds_full = make_ds(**input_dataset_args, df=df, concat_tops=True)\n",
    "ds_full_valid = make_ds(**input_dataset_args, df=df_valid, concat_tops=True)\n",
    "\n",
    "ds_curbars = make_ds(**input_dataset_args, df=df[['c_eur', 'c_gbp', 'c_jpy']], concat_tops=True)\n",
    "ds_curbars_valid = make_ds(**input_dataset_args, df=df_valid[['c_eur', 'c_gbp', 'c_jpy']], concat_tops=True)\n",
    "\n",
    "ds_event_exist = make_ds(**input_dataset_args, df=df['event_exist'])\n",
    "ds_event_exist_valid = make_ds(**input_dataset_args, df=df_valid['event_exist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dataset_args = {'batch_size':batch_size,\n",
    "                      'time_steps':train_size,\n",
    "                      'shift':shift, \n",
    "                      'skip_steps':1}\n",
    "\n",
    "ds_future = make_ds(**future_dataset_args, df=df[['event_exist', 'month', 'dow', 'hour', 'event_cur']], concat_tops=True)\n",
    "ds_future_valid = make_ds(**future_dataset_args, df=df_valid[['event_exist', 'month', 'dow', 'hour', 'event_cur']], concat_tops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_label  <BatchDataset shapes: (64, None, 3), types: tf.float64>\n",
      "ds_full  <BatchDataset shapes: (64, None, 432), types: tf.float64>\n",
      "ds_curbars  <BatchDataset shapes: (64, None, 36), types: tf.float64>\n",
      "ds_event_exist  <BatchDataset shapes: (64, None, 70), types: tf.float64>\n",
      "ds_future  <BatchDataset shapes: (64, None, 116), types: tf.float64>\n"
     ]
    }
   ],
   "source": [
    "print('ds_label ',ds_label)\n",
    "\n",
    "print('ds_full ', ds_full)\n",
    "print('ds_curbars ', ds_curbars)\n",
    "print('ds_event_exist ', ds_event_exist)\n",
    "\n",
    "print('ds_future ', ds_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedActivationUnit(keras.layers.Layer):\n",
    "    def __init__(self, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        n_filters = inputs.shape[-1] // 2\n",
    "        linear_output = self.activation(inputs[..., :n_filters])\n",
    "        gate = keras.activations.sigmoid(inputs[..., n_filters:])\n",
    "        return self.activation(linear_output) * gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavenet_residual_block(inputs, n_filters, dilation_rate):\n",
    "    z = keras.layers.Conv1D(2 * n_filters, kernel_size=2, padding=\"causal\",\n",
    "                            dilation_rate=dilation_rate)(inputs)\n",
    "    z = GatedActivationUnit()(z)\n",
    "    z = keras.layers.Conv1D(n_filters, kernel_size=1)(z)\n",
    "    return keras.layers.Add()([z, inputs]), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavenet_model_setup(n_layers_per_block, n_blocks, n_filters, n_outputs, feature_dim, name):\n",
    "    # n_layers_per_block = 10 in the paper\n",
    "    # n_blocks = 3 in the paper\n",
    "    # n_filters = 128 in the paper\n",
    "    # n_outputs = 256 in the paper\n",
    "    \n",
    "    inputs = keras.layers.Input(shape=[train_size, feature_dim]) # 164 with cross model\n",
    "    z = keras.layers.Conv1D(n_filters, kernel_size=2, padding=\"causal\")(inputs)\n",
    "    skip_to_last = []\n",
    "    for dilation_rate in [2**i for i in range(n_layers_per_block)] * n_blocks:\n",
    "        z, skip = wavenet_residual_block(z, n_filters, dilation_rate)\n",
    "        skip_to_last.append(skip)\n",
    "    z = keras.activations.relu(keras.layers.Add()(skip_to_last))\n",
    "    z = keras.layers.Conv1D(n_filters, kernel_size=1, activation=\"relu\")(z)\n",
    "    out = keras.layers.Conv1D(n_outputs, kernel_size=1, activation=\"relu\")(z)\n",
    "    return keras.models.Model(inputs=[inputs], outputs=[out], name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_event_exist_setup(inputs, embed_dim, name=None):\n",
    "    out = keras.layers.Conv1D(embed_dim, kernel_size=1, strides=1, padding='valid')(inputs)\n",
    "    return keras.models.Model(inputs=[inputs], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "input_event_exist = keras.layers.Input(shape=[train_size, 70], name = 'input_event_exist') \n",
    "input_curbars = keras.layers.Input(shape=[train_size, 36], name = 'input_curbars')\n",
    "input_full = keras.layers.Input(shape=[train_size, 432], name = 'input_full')\n",
    "\n",
    "input_future = keras.layers.Input(shape=[train_size, 116], name = 'input_future')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Simplest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mod_wavenet_fl64_fd36\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 144, 36)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 144, 64)      4672        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 144, 128)     16512       conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_8 (GatedA (None, 144, 64)      0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 144, 64)      4160        gated_activation_unit_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 144, 64)      0           conv1d_21[0][0]                  \n",
      "                                                                 conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 144, 128)     16512       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_9 (GatedA (None, 144, 64)      0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 144, 64)      4160        gated_activation_unit_9[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 144, 64)      0           conv1d_23[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 144, 128)     16512       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_10 (Gated (None, 144, 64)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 144, 64)      4160        gated_activation_unit_10[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 144, 64)      0           conv1d_25[0][0]                  \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 144, 128)     16512       add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_11 (Gated (None, 144, 64)      0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 144, 64)      4160        gated_activation_unit_11[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 144, 64)      0           conv1d_27[0][0]                  \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 144, 128)     16512       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_12 (Gated (None, 144, 64)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 144, 64)      4160        gated_activation_unit_12[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 144, 64)      0           conv1d_29[0][0]                  \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 144, 128)     16512       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_13 (Gated (None, 144, 64)      0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 144, 64)      4160        gated_activation_unit_13[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 144, 64)      0           conv1d_31[0][0]                  \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 144, 128)     16512       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_14 (Gated (None, 144, 64)      0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 144, 64)      4160        gated_activation_unit_14[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 144, 64)      0           conv1d_33[0][0]                  \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 144, 128)     16512       add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_15 (Gated (None, 144, 64)      0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 144, 64)      4160        gated_activation_unit_15[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 144, 64)      0           conv1d_21[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_25[0][0]                  \n",
      "                                                                 conv1d_27[0][0]                  \n",
      "                                                                 conv1d_29[0][0]                  \n",
      "                                                                 conv1d_31[0][0]                  \n",
      "                                                                 conv1d_33[0][0]                  \n",
      "                                                                 conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_1 (TensorFlowO [(None, 144, 64)]    0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 144, 64)      4160        tf_op_layer_Relu_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 144, 3)       195         conv1d_36[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 174,403\n",
      "Trainable params: 174,403\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_wn = wavenet_model_setup(n_layers_per_block=4, n_blocks=2, n_filters=64, n_outputs=3, feature_dim=36, name='mod_wavenet_fl64_fd36')\n",
    "model_wn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_curbars (InputLayer)   [(None, 144, 36)]         0         \n",
      "_________________________________________________________________\n",
      "mod_wavenet_fl64_fd36 (Model (None, 144, 3)            174403    \n",
      "=================================================================\n",
      "Total params: 174,403\n",
      "Trainable params: 174,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wave_out = model_wn(input_curbars)\n",
    "\n",
    "model_no_f = keras.models.Model(inputs=[input_curbars], outputs=[wave_out])\n",
    "\n",
    "model_no_f.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.zip((ds_curbars, ds_label)).shuffle(128, reshuffle_each_iteration=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_valid = tf.data.Dataset.zip((ds_curbars_valid, ds_label_valid)).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "3500/3500 [==============================] - 198s 56ms/step - loss: 6.6098e-04 - mae: 0.0130 - val_loss: 5.4135e-05 - val_mae: 0.0068\n",
      "Epoch 2/14\n",
      "3500/3500 [==============================] - 187s 53ms/step - loss: 6.8699e-05 - mae: 0.0065 - val_loss: 1.2946e-05 - val_mae: 0.0038\n",
      "Epoch 3/14\n",
      "3500/3500 [==============================] - 183s 52ms/step - loss: 4.7091e-05 - mae: 0.0052 - val_loss: 2.9264e-05 - val_mae: 0.0066\n",
      "Epoch 4/14\n",
      "3500/3500 [==============================] - 183s 52ms/step - loss: 4.2217e-05 - mae: 0.0048 - val_loss: 5.3478e-06 - val_mae: 0.0027\n",
      "Epoch 5/14\n",
      "3500/3500 [==============================] - 183s 52ms/step - loss: 4.1491e-05 - mae: 0.0046 - val_loss: 9.8578e-06 - val_mae: 0.0034\n",
      "Epoch 6/14\n",
      "3500/3500 [==============================] - 185s 53ms/step - loss: 3.8956e-05 - mae: 0.0048 - val_loss: 7.2391e-06 - val_mae: 0.0027\n",
      "Epoch 7/14\n",
      "3500/3500 [==============================] - 188s 54ms/step - loss: 3.4971e-05 - mae: 0.0047 - val_loss: 7.8174e-06 - val_mae: 0.0027\n",
      "Epoch 8/14\n",
      "3500/3500 [==============================] - 189s 54ms/step - loss: 2.1820e-05 - mae: 0.0037 - val_loss: 4.1769e-06 - val_mae: 0.0020\n",
      "Epoch 9/14\n",
      "3500/3500 [==============================] - 186s 53ms/step - loss: 2.1643e-05 - mae: 0.0035 - val_loss: 1.4186e-06 - val_mae: 0.0011\n",
      "Epoch 10/14\n",
      "3500/3500 [==============================] - 187s 53ms/step - loss: 1.9324e-05 - mae: 0.0035 - val_loss: 4.0832e-06 - val_mae: 0.0019\n",
      "Epoch 11/14\n",
      "3500/3500 [==============================] - 186s 53ms/step - loss: 1.4039e-05 - mae: 0.0029 - val_loss: 1.4967e-06 - val_mae: 0.0013\n",
      "Epoch 12/14\n",
      "3500/3500 [==============================] - 188s 54ms/step - loss: 1.4472e-05 - mae: 0.0028 - val_loss: 2.0659e-06 - val_mae: 0.0017\n",
      "Epoch 13/14\n",
      "3500/3500 [==============================] - 188s 54ms/step - loss: 1.2220e-05 - mae: 0.0030 - val_loss: 1.6580e-06 - val_mae: 0.0013\n",
      "Epoch 14/14\n",
      "3500/3500 [==============================] - 190s 54ms/step - loss: 1.1686e-05 - mae: 0.0027 - val_loss: 4.3141e-06 - val_mae: 0.0023\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_no_f.compile(loss=keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model_wn_no_f.h5\", save_best_only=True)\n",
    "\n",
    "history = model_no_f.fit(ds_train, epochs=14,\n",
    "                       validation_data=ds_valid,\n",
    "                       callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Simple Model with all input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_event_exist (InputLaye [(None, 144, 70)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 144, 6)            426       \n",
      "=================================================================\n",
      "Total params: 426\n",
      "Trainable params: 426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_event_exist = model_event_exist_setup(input_event_exist, 6, name='mod_event_exist')\n",
    "model_event_exist.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wn = wavenet_model_setup(n_layers_per_block=4, n_blocks=2, n_filters=64, n_outputs=3, feature_dim=42, name='mod_wavenet_fl64_fd42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_event_exist (InputLayer)  [(None, 144, 70)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_curbars (InputLayer)      [(None, 144, 36)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 144, 6)       426         input_event_exist[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 144, 42)      0           input_curbars[0][0]              \n",
      "                                                                 model_1[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mod_wavenet_fl64_fd42 (Model)   (None, 144, 3)       175171      concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 175,597\n",
      "Trainable params: 175,597\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_event_exist = model_event_exist(input_event_exist)\n",
    "\n",
    "input_wn = keras.layers.Concatenate(axis=-1)([input_curbars, emb_event_exist])\n",
    "\n",
    "wave_out = model_wn(input_wn)\n",
    "\n",
    "model_no_f_ev_ex = keras.models.Model(inputs=[input_curbars, input_event_exist], outputs=[wave_out])\n",
    "\n",
    "model_no_f_ev_ex.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.zip(((ds_curbars, ds_event_exist), ds_label)).shuffle(128, reshuffle_each_iteration=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_valid = tf.data.Dataset.zip(((ds_curbars_valid, ds_event_exist_valid), ds_label_valid)).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "3500/3500 [==============================] - 297s 85ms/step - loss: 8.8564e-04 - mae: 0.0133 - val_loss: 7.0183e-05 - val_mae: 0.0097\n",
      "Epoch 2/14\n",
      "3500/3500 [==============================] - 291s 83ms/step - loss: 7.2995e-05 - mae: 0.0061 - val_loss: 7.5863e-06 - val_mae: 0.0029\n",
      "Epoch 3/14\n",
      "3500/3500 [==============================] - 287s 82ms/step - loss: 7.4844e-05 - mae: 0.0061 - val_loss: 1.2169e-05 - val_mae: 0.0034\n",
      "Epoch 4/14\n",
      "3500/3500 [==============================] - 290s 83ms/step - loss: 4.7924e-05 - mae: 0.0046 - val_loss: 1.5435e-05 - val_mae: 0.0035\n",
      "Epoch 5/14\n",
      "3500/3500 [==============================] - 289s 83ms/step - loss: 3.3789e-05 - mae: 0.0046 - val_loss: 9.7458e-06 - val_mae: 0.0029\n",
      "Epoch 6/14\n",
      "3500/3500 [==============================] - 287s 82ms/step - loss: 3.3019e-05 - mae: 0.0047 - val_loss: 9.6791e-06 - val_mae: 0.0032\n",
      "Epoch 7/14\n",
      "3500/3500 [==============================] - 281s 80ms/step - loss: 2.0366e-05 - mae: 0.0037 - val_loss: 9.3054e-06 - val_mae: 0.0031\n",
      "Epoch 8/14\n",
      "3500/3500 [==============================] - 292s 83ms/step - loss: 1.4734e-05 - mae: 0.0034 - val_loss: 1.1756e-06 - val_mae: 0.0010\n",
      "Epoch 9/14\n",
      "3500/3500 [==============================] - 289s 82ms/step - loss: 1.5409e-05 - mae: 0.0034 - val_loss: 2.9415e-06 - val_mae: 0.0021\n",
      "Epoch 10/14\n",
      "3500/3500 [==============================] - 291s 83ms/step - loss: 1.3886e-05 - mae: 0.0031 - val_loss: 8.1084e-06 - val_mae: 0.0035\n",
      "Epoch 11/14\n",
      "3500/3500 [==============================] - 289s 83ms/step - loss: 1.8629e-05 - mae: 0.0034 - val_loss: 2.7960e-06 - val_mae: 0.0018\n",
      "Epoch 12/14\n",
      "3500/3500 [==============================] - 287s 82ms/step - loss: 1.4109e-05 - mae: 0.0032 - val_loss: 2.4003e-06 - val_mae: 0.0016\n",
      "Epoch 13/14\n",
      "3500/3500 [==============================] - 288s 82ms/step - loss: 1.0654e-05 - mae: 0.0028 - val_loss: 7.2603e-07 - val_mae: 8.7632e-04\n",
      "Epoch 14/14\n",
      "3500/3500 [==============================] - 290s 83ms/step - loss: 1.1846e-05 - mae: 0.0030 - val_loss: 3.4609e-06 - val_mae: 0.0022\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_no_f_ev_ex.compile(loss=keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model_nofut_fullin.h5\", save_best_only=True)\n",
    "\n",
    "history = model_no_f_ev_ex.fit(ds_train, epochs=14,\n",
    "                       validation_data=ds_valid,\n",
    "                       callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with price and return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_out = model_wn(input_full)\n",
    "\n",
    "'''\n",
    "input_f = keras.layers.Conv1D(32, kernel_size=1, strides=1, padding='valid', activation='relu')(input_future)\n",
    "\n",
    "ftr = keras.layers.Concatenate(axis=-1)([out_timed, input_f])\n",
    "ftr = keras.layers.Conv1D(24, kernel_size=1, strides=1, padding='valid', activation='relu')(ftr)\n",
    "ftr = keras.layers.BatchNormalization()(ftr)\n",
    "out_f = keras.layers.Conv1D(3, kernel_size=1, strides=1, padding='valid', activation=None)(ftr)\n",
    "'''\n",
    "\n",
    "model_no_f = keras.models.Model(inputs=[input_full], outputs=[wave_out])\n",
    "\n",
    "model_no_f.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "   3500/Unknown - 209s 60ms/step - loss: 0.3232 - mae: 0.5002 - mae_mean_first: 0.4993"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to create link (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-f17b2ea24610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m history = model_sum.fit(ds_train, epochs=14,\n\u001b[1;32m      6\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                        callbacks=[checkpoint_cb])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                       total_epochs=1)\n\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 397\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    769\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m       \u001b[0;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1027\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \"\"\"\n\u001b[1;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1008\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 112\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    113\u001b[0m     if (include_optimizer and model.optimizer and\n\u001b[1;32m    114\u001b[0m         not isinstance(model.optimizer, optimizers.TFOptimizer)):\n\u001b[0;32m--> 115\u001b[0;31m       \u001b[0msave_optimizer_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_optimizer_weights_to_hdf5_group\u001b[0;34m(hdf5_group, optimizer)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m       param_dset = weights_group.create_dataset(\n\u001b[0;32m--> 587\u001b[0;31m           name, val.shape, dtype=val.dtype)\n\u001b[0m\u001b[1;32m    588\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to create link (name already exists)"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_sum.compile(loss=weighted_mae_loss_setup(5), optimizer=optimizer, metrics=[\"mae\", mae_mean_first])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model_wn_simple.h5\", save_best_only=True)\n",
    "\n",
    "history = model_sum.fit(ds_train, epochs=14,\n",
    "                       validation_data=ds_valid,\n",
    "                       callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_direct = keras.models.load_model(\"model_wn_no_f.h5\", custom_objects={'GatedActivationUnit' : GatedActivationUnit})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_direct.evaluate(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_direct.predict(ds_train.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 144, 3), dtype=float64, numpy=\n",
       "array([[[1.14660213, 1.27502506, 0.91176042],\n",
       "        [1.14665078, 1.27506083, 0.91166034],\n",
       "        [1.14665252, 1.2750439 , 0.91146625],\n",
       "        ...,\n",
       "        [1.14387721, 1.26764373, 0.91705545],\n",
       "        [1.14373084, 1.26736633, 0.91678977],\n",
       "        [1.14366858, 1.26727479, 0.9167313 ]],\n",
       "\n",
       "       [[1.14665078, 1.27506083, 0.91166034],\n",
       "        [1.14665252, 1.2750439 , 0.91146625],\n",
       "        [1.14670331, 1.27497803, 0.91151996],\n",
       "        ...,\n",
       "        [1.14373084, 1.26736633, 0.91678977],\n",
       "        [1.14366858, 1.26727479, 0.9167313 ],\n",
       "        [1.14336997, 1.2670677 , 0.91643917]],\n",
       "\n",
       "       [[1.14665252, 1.2750439 , 0.91146625],\n",
       "        [1.14670331, 1.27497803, 0.91151996],\n",
       "        [1.14674477, 1.27492272, 0.91152943],\n",
       "        ...,\n",
       "        [1.14366858, 1.26727479, 0.9167313 ],\n",
       "        [1.14336997, 1.2670677 , 0.91643917],\n",
       "        [1.14283442, 1.2668242 , 0.91627286]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.14452395, 1.27308128, 0.91436697],\n",
       "        [1.14446863, 1.27306422, 0.9144089 ],\n",
       "        [1.14450592, 1.27307453, 0.91439593],\n",
       "        ...,\n",
       "        [1.13540444, 1.26051318, 0.91475577],\n",
       "        [1.13562099, 1.2606274 , 0.91498548],\n",
       "        [1.13594893, 1.26083678, 0.91475205]],\n",
       "\n",
       "       [[1.14446863, 1.27306422, 0.9144089 ],\n",
       "        [1.14450592, 1.27307453, 0.91439593],\n",
       "        [1.14457449, 1.2732469 , 0.91444103],\n",
       "        ...,\n",
       "        [1.13562099, 1.2606274 , 0.91498548],\n",
       "        [1.13594893, 1.26083678, 0.91475205],\n",
       "        [1.1359395 , 1.26101237, 0.91486199]],\n",
       "\n",
       "       [[1.14450592, 1.27307453, 0.91439593],\n",
       "        [1.14457449, 1.2732469 , 0.91444103],\n",
       "        [1.14437862, 1.27318488, 0.9143341 ],\n",
       "        ...,\n",
       "        [1.13594893, 1.26083678, 0.91475205],\n",
       "        [1.1359395 , 1.26101237, 0.91486199],\n",
       "        [1.13556672, 1.25915852, 0.91498571]]])>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ds_label_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15000 is out of bounds for axis 0 with size 192",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b2c07b0034db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 15000 is out of bounds for axis 0 with size 192"
     ]
    }
   ],
   "source": [
    "pred[15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>c_eur</th>\n",
       "      <th>c_gbp</th>\n",
       "      <th>c_jpy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean_log_r</th>\n",
       "      <th>mean_log_r</th>\n",
       "      <th>mean_log_r</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-02-19 15:20:00</th>\n",
       "      <td>0.356657</td>\n",
       "      <td>0.460169</td>\n",
       "      <td>-0.382856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 15:25:00</th>\n",
       "      <td>1.451761</td>\n",
       "      <td>-0.149109</td>\n",
       "      <td>1.167526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 15:30:00</th>\n",
       "      <td>-0.762259</td>\n",
       "      <td>1.091631</td>\n",
       "      <td>-0.608134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 15:35:00</th>\n",
       "      <td>-1.157844</td>\n",
       "      <td>0.840024</td>\n",
       "      <td>-0.814383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 15:40:00</th>\n",
       "      <td>0.138421</td>\n",
       "      <td>0.492306</td>\n",
       "      <td>0.455284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 15:45:00</th>\n",
       "      <td>0.316235</td>\n",
       "      <td>-0.310320</td>\n",
       "      <td>0.520709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 15:50:00</th>\n",
       "      <td>0.403279</td>\n",
       "      <td>-0.080827</td>\n",
       "      <td>-0.010239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 15:55:00</th>\n",
       "      <td>-0.433683</td>\n",
       "      <td>-1.497703</td>\n",
       "      <td>0.609927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:00:00</th>\n",
       "      <td>0.698208</td>\n",
       "      <td>1.067084</td>\n",
       "      <td>2.494162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:05:00</th>\n",
       "      <td>2.826195</td>\n",
       "      <td>3.901023</td>\n",
       "      <td>2.827525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:10:00</th>\n",
       "      <td>0.054438</td>\n",
       "      <td>1.938339</td>\n",
       "      <td>-0.950027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:15:00</th>\n",
       "      <td>-0.098041</td>\n",
       "      <td>1.269471</td>\n",
       "      <td>-0.939383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:20:00</th>\n",
       "      <td>-0.485571</td>\n",
       "      <td>2.053507</td>\n",
       "      <td>-0.184540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:25:00</th>\n",
       "      <td>1.087414</td>\n",
       "      <td>0.561953</td>\n",
       "      <td>0.684165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:30:00</th>\n",
       "      <td>-0.554294</td>\n",
       "      <td>1.717584</td>\n",
       "      <td>0.000649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:35:00</th>\n",
       "      <td>0.626310</td>\n",
       "      <td>0.181379</td>\n",
       "      <td>1.684470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:40:00</th>\n",
       "      <td>-0.427910</td>\n",
       "      <td>1.534414</td>\n",
       "      <td>-0.039731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:45:00</th>\n",
       "      <td>0.812024</td>\n",
       "      <td>0.147083</td>\n",
       "      <td>1.969139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:50:00</th>\n",
       "      <td>0.054966</td>\n",
       "      <td>-0.670165</td>\n",
       "      <td>-0.046921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-19 16:55:00</th>\n",
       "      <td>0.405855</td>\n",
       "      <td>0.836889</td>\n",
       "      <td>1.364507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         c_eur      c_gbp      c_jpy\n",
       "                    mean_log_r mean_log_r mean_log_r\n",
       "ctime                                               \n",
       "2016-02-19 15:20:00   0.356657   0.460169  -0.382856\n",
       "2016-02-19 15:25:00   1.451761  -0.149109   1.167526\n",
       "2016-02-19 15:30:00  -0.762259   1.091631  -0.608134\n",
       "2016-02-19 15:35:00  -1.157844   0.840024  -0.814383\n",
       "2016-02-19 15:40:00   0.138421   0.492306   0.455284\n",
       "2016-02-19 15:45:00   0.316235  -0.310320   0.520709\n",
       "2016-02-19 15:50:00   0.403279  -0.080827  -0.010239\n",
       "2016-02-19 15:55:00  -0.433683  -1.497703   0.609927\n",
       "2016-02-19 16:00:00   0.698208   1.067084   2.494162\n",
       "2016-02-19 16:05:00   2.826195   3.901023   2.827525\n",
       "2016-02-19 16:10:00   0.054438   1.938339  -0.950027\n",
       "2016-02-19 16:15:00  -0.098041   1.269471  -0.939383\n",
       "2016-02-19 16:20:00  -0.485571   2.053507  -0.184540\n",
       "2016-02-19 16:25:00   1.087414   0.561953   0.684165\n",
       "2016-02-19 16:30:00  -0.554294   1.717584   0.000649\n",
       "2016-02-19 16:35:00   0.626310   0.181379   1.684470\n",
       "2016-02-19 16:40:00  -0.427910   1.534414  -0.039731\n",
       "2016-02-19 16:45:00   0.812024   0.147083   1.969139\n",
       "2016-02-19 16:50:00   0.054966  -0.670165  -0.046921\n",
       "2016-02-19 16:55:00   0.405855   0.836889   1.364507"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label[10000:10020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Simple with decoupled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.zip(((ds_bars_simple, ds_event_ohe, ds_time_cur, ds_event_exist, ds_time_curr_future, ds_event_future), \n",
    "                                 ds_label_means)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_valid = tf.data.Dataset.zip(((ds_bars_simple_valid, ds_event_ohe_valid, ds_time_cur_valid, ds_event_exist_valid, ds_time_curr_future_valid, ds_event_future_valid), \n",
    "                                 ds_label_means_valid)).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future and past embeds are different\n",
    "model_time_cur_f = model_time_cur_setup(input_f_time_cur, 6, name='model_time_cur_f')\n",
    "model_event_exist_f = model_event_exist_setup(input_f_event_exist, 10, name='model_event_exist_f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = model_event_ohe(input_ohe)\n",
    "time_cur = model_time_cur(input_time_cur)\n",
    "event_ex = model_event_exist(input_event_exist)\n",
    "inputs = keras.layers.Concatenate(axis=-1)([input_simple_c, ohe, time_cur, event_ex])\n",
    "wave = model_wn(inputs)\n",
    "out_head = model_time_transform(wave)\n",
    "\n",
    "time_cur_f = model_time_cur_f(input_f_time_cur)\n",
    "event_ex_f = model_event_exist_f(input_f_event_exist)\n",
    "\n",
    "input_f = keras.layers.Concatenate(axis=-1)([out_head, time_cur_f, event_ex_f])\n",
    "out_f = keras.layers.Conv1D(3, kernel_size=1, strides=1, padding='valid')(input_f)\n",
    "\n",
    "model_sum = keras.models.Model(inputs=[input_simple_c, input_ohe, input_time_cur, input_event_exist, input_f_time_cur, input_f_event_exist], outputs=[out_f])\n",
    "\n",
    "model_sum.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model_sum.compile(loss=keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\", mean_metric(last_index=None)])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model_wn_decoup.h5\", save_best_only=True)\n",
    "\n",
    "history = model_sum.fit(ds_train, epochs=14,\n",
    "                       validation_data=ds_valid,\n",
    "                       callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redefine Head Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_model2_setup():\n",
    "    inputs = keras.layers.Input(shape=[train_size, 3])\n",
    "    z = keras.layers.BatchNormalization()(inputs)\n",
    "    z = keras.layers.Conv1D(8, kernel_size=5, strides=3, padding='valid', activation='relu')(z)\n",
    "    z = keras.layers.BatchNormalization()(z)\n",
    "    z = keras.layers.Conv1D(16, kernel_size=3, strides=2, padding='valid', activation='relu')(z)\n",
    "    z = keras.layers.BatchNormalization()(z)\n",
    "    z = keras.layers.Conv1D(32, kernel_size=3, strides=2, padding='valid', activation='relu')(z)\n",
    "    out = keras.layers.BatchNormalization()(z)\n",
    "    return keras.models.Model(inputs=[inputs], outputs=[out], name = 'model_head2')\n",
    "\n",
    "model_head2 = head_model2_setup()\n",
    "model_head2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottleneck?\n",
    "model_event_ohe = model_event_ohe_setup(input_ohe, 64)\n",
    "model_event_ohe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +48 features\n",
    "model_wn = wavenet_model_setup(n_layers_per_block=3, n_blocks=2, n_filters=64, n_outputs=3, feature_dim=113, name='model_wn_fl64_fd81')\n",
    "# model_wn_out = wavenet_model_setup(n_layers_per_block=3, n_blocks=1, n_filters=64, n_outputs=3, feature_dim=48, name='model_wn_out_fl64_fd_48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size=71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_label = make_ds(batch_size=batch_size,\n",
    "                   time_steps=target_size, \n",
    "                   shift=shift, \n",
    "                   skip_steps=train_size, \n",
    "                   df=df_label)\n",
    "\n",
    "ds_label_valid = make_ds(batch_size=batch_size,\n",
    "                   time_steps=target_size, \n",
    "                   shift=shift, \n",
    "                   skip_steps=train_size, \n",
    "                   df=df_label_valid)\n",
    "ds_event_future = make_ds(  batch_size=batch_size,\n",
    "                            time_steps=target_size, \n",
    "                            shift=shift, \n",
    "                            skip_steps=train_size, \n",
    "                            df=df['event_exist'])\n",
    "ds_time_curr_future = make_ds(  batch_size=batch_size,\n",
    "                            time_steps=target_size, \n",
    "                            shift=shift, \n",
    "                            skip_steps=train_size, \n",
    "                            df=df[['month', 'dow', 'hour', 'event_cur']], concat_tops=True)\n",
    "\n",
    "ds_event_future_valid = make_ds(  batch_size=batch_size,\n",
    "                            time_steps=target_size, \n",
    "                            shift=shift, \n",
    "                            skip_steps=train_size, \n",
    "                            df=df_valid['event_exist'])\n",
    "ds_time_curr_future_valid = make_ds(  batch_size=batch_size,\n",
    "                            time_steps=target_size, \n",
    "                            shift=shift, \n",
    "                            skip_steps=train_size, \n",
    "                            df=df_valid[['month', 'dow', 'hour', 'event_cur']], concat_tops=True)\n",
    "\n",
    "input_f_time_cur = keras.layers.Input(shape=[target_size, 46], name = 'input_f_time_cur')\n",
    "input_f_event_exist = keras.layers.Input(shape=[target_size, 70], name = 'input_f_event_exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = model_event_ohe(input_ohe)\n",
    "time_cur = model_time_cur(input_time_cur)\n",
    "event_ex = model_event_exist(input_event_exist)\n",
    "inputs = keras.layers.Concatenate(axis=-1)([input_simple_c, ohe, time_cur, event_ex])\n",
    "wave = model_wn(inputs)\n",
    "out_head = model_head2(wave)\n",
    "\n",
    "time_cur_f = model_time_cur(input_f_time_cur)\n",
    "event_ex_f = model_event_exist(input_f_event_exist)\n",
    "\n",
    "input_f = keras.layers.Concatenate(axis=-1)([out_head, time_cur_f, event_ex_f])\n",
    "input_f = keras.layers.Conv1D(48, kernel_size=3, strides=1, padding='causal', activation='relu')(input_f)\n",
    "out_f = keras.layers.Conv1D(3, kernel_size=1, strides=1, padding='valid')(input_f)\n",
    "\n",
    "model_sum = keras.models.Model(inputs=[input_simple_c, input_ohe, input_time_cur, input_event_exist, input_f_time_cur, input_f_event_exist], outputs=[out_f])\n",
    "\n",
    "model_sum.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if model converge\n",
    "'''\n",
    "ds_worker_test = tf.data.Dataset.zip(((ds_bars_simple, ds_event_ohe, ds_time_cur, ds_event_exist, ds_time_curr_future, ds_event_future), \n",
    "                                 ds_label)).shuffle(64).prefetch(tf.data.experimental.AUTOTUNE).take(64)\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model_sum.compile(loss=keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n",
    "history = model_sum.fit(ds_worker_test, epochs=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.zip(((ds_bars_simple, ds_event_ohe, ds_time_cur, ds_event_exist, ds_time_curr_future, ds_event_future), \n",
    "                                 ds_label)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_valid = tf.data.Dataset.zip(((ds_bars_simple_valid, ds_event_ohe_valid, ds_time_cur_valid, ds_event_exist_valid, ds_time_curr_future_valid, ds_event_future_valid), \n",
    "                                 ds_label_valid)).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model_sum.compile(loss=keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model_wn_64_head_2.h5\", save_best_only=True)\n",
    "\n",
    "history = model_sum.fit(ds_train, epochs=20,\n",
    "                       validation_data=ds_valid,\n",
    "                       callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cross Feature model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "z1 = keras.layers.Conv2D(2*3, kernel_size=1, padding=\"valid\")(input_c1)\n",
    "z1 = keras.layers.Reshape([train_size,66])(z1)\n",
    "\n",
    "z2 = keras.layers.Conv2D(2*11, kernel_size=1, padding=\"valid\")(input_c2)\n",
    "z2 = keras.layers.Reshape([train_size,66])(z2)\n",
    "\n",
    "out = keras.layers.Concatenate(axis=-1)([z1, z2])\n",
    "model_cur_cross_in = keras.models.Model(inputs=[input_c1, input_c2], outputs=[out])\n",
    "model_cur_cross_in._name = 'model_cur_cross_in'\n",
    "\n",
    "model_cur_cross_in.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wn = wavenet_model(n_layers_per_block=3, n_blocks=2, n_filters=256, n_outputs=3, feature_dim=164)\n",
    "# (fast test) #  model_wn = wavenet_model(n_layers_per_block=2, n_blocks=1, n_filters=1, n_outputs=3)\n",
    "model_wn._name = 'model_wavenet'\n",
    "\n",
    "#keras.utils.plot_model(model, show_shapes=True)\n",
    "model_wn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = model_event_ohe(input_ohe)\n",
    "time_cur = model_time_cur(input_time_cur)\n",
    "event_ex = model_event_exist(input_event_exist)\n",
    "cur_cross = model_cur_cross_in([input_c1, input_c2])\n",
    "inputs = keras.layers.Concatenate(axis=-1)([cur_cross, ohe, time_cur, event_ex])\n",
    "wave = model_wn(inputs)\n",
    "out_head = model_head(wave)\n",
    "\n",
    "time_cur_f = model_time_cur(input_f_time_cur)\n",
    "event_ex_f = model_event_exist(input_f_event_exist)\n",
    "\n",
    "input_f = keras.layers.Concatenate(axis=-1)([out_head, time_cur_f, event_ex_f])\n",
    "out_f = keras.layers.Conv1D(3, kernel_size=1, strides=1, padding='valid')(input_f)\n",
    "\n",
    "model_sum = keras.models.Model(inputs=[input_c1, input_c2, input_ohe, input_time_cur, input_event_exist, input_f_time_cur, input_f_event_exist], outputs=[out_f])\n",
    "\n",
    "model_sum.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = keras.layers.Conv1D(64, kernel_size=1, strides=1, padding='valid')(input_ohe)\n",
    "model_event_ohe = keras.models.Model(inputs=[input_ohe], outputs=[out])\n",
    "model_event_ohe._name = 'model_event_ohe'\n",
    "\n",
    "model_event_ohe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_transform = dict()\n",
    "# standardize features based on data up to 2019-01-01 00:00\n",
    "for cur in c_list:\n",
    "    mean_to_19 = df_label[cur, 'mean'].loc['2016':'2019'].mean()\n",
    "    std_to_19 = df_label[cur, 'mean'].loc['2016':'2019'].std()\n",
    "    label_transform.update({(cur, 'mean') : {'column_mean' : mean_to_19, 'column_std' : std_to_19}})\n",
    "    #df_label.loc[:,(cur, 'mean')] = (df_label.loc[:,(cur, 'mean')] - mean_to_19)/std_to_19"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
